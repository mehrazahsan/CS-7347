# An Overview of Sentiments Expressed Across Abstractive and Extractive Summaries
## By: Mohammed Ahsan, Carter Mondy
Auto summarization is one of the premier tasks in natural language processing, and as more and more literature is published digitally the need for robust summarization methods is only increasing. Our work here compares the performance and nuances of abstractive and extractive summarization methods. We analyzed summary similarity as well as summary sentiment compared to the original text. To perform this analysis, we compared two models, BERT Extractive Summarizer and GPT for both abstractive and extractive text summarization.  Our dataset is the CNN/Daily Mail dataset that contains various text samples as well as their human generated summaries. We used a RoBERTa model to create the sentiment scores for the original text, human summary, and model generated summaries, and then compared the results. We also used ROGUE scores to quantify the similarity between the original text and model generated summaries. The goal of this work is to help us understand the difference between these two text summarization methods with quantifiable results to accurately measure performance. Our work found that extractive summarizations tend to preserve the original sentiment and lexical accuracy better than abstractive summarization. However, we examined the caveats with this approach as well as potential improvements to the evaluation techniques.  
